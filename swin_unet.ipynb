{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80b988e1-ff36-4288-89e8-626faa540951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57355e02-3b7b-4c29-bc6b-6d11c4844268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (size,size))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image = image/255.\n",
    "    return image\n",
    "\n",
    "def load_data(root_path, size):\n",
    "    images = []\n",
    "    masks = []\n",
    "    x = 0\n",
    "    for path in sorted(glob(root_path)):\n",
    "        img = load_image(path, size)\n",
    "        if 'mask' in path:\n",
    "            if x:\n",
    "                masks[-1] += img\n",
    "                masks[-1] = np.array(masks[-1]>0.5, dtype='float64')\n",
    "            else:\n",
    "                masks.append(img)\n",
    "                x = 1\n",
    "        else:\n",
    "            images.append(img)\n",
    "            x = 0\n",
    "    return np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542385f-70d3-47ef-9f4d-2fbb638fc32a",
   "metadata": {},
   "source": [
    "## Loading image and mask data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a76e0c9-2ec8-4c48-9f56-e512ba0a6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def load_image(path, size):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (size, size))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def load_data(image_folder, mask_folder, size):\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    # Get sorted file lists\n",
    "    image_paths = sorted(glob(os.path.join(image_folder, '*')))\n",
    "    mask_paths = sorted(glob(os.path.join(mask_folder, '*')))\n",
    "    \n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        # Load image\n",
    "        img = load_image(img_path, size)\n",
    "        images.append(img)\n",
    "        \n",
    "        # Load mask\n",
    "        mask = load_image(mask_path, size)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "038b74ef-3380-4911-a767-22b07ea558d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5474, 224, 224),\n",
       " (1173, 224, 224),\n",
       " (1173, 224, 224),\n",
       " (5474, 224, 224),\n",
       " (1173, 224, 224),\n",
       " (1173, 224, 224))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"Your designated Image path\"\n",
    "masks_path = \"Your designated mask path\"\n",
    "\n",
    "size = 224\n",
    "X, Y = load_data(image_path, masks_path, size)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, shuffle=True)\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size=0.5,shuffle=True)\n",
    "X_train.shape, X_valid.shape, X_test.shape, Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41cf39db-7cba-4093-8cc3-1544c1f2714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27370, 1, 224, 224]),\n",
       " torch.Size([1173, 1, 224, 224]),\n",
       " torch.Size([1173, 1, 224, 224]),\n",
       " torch.Size([27370, 1, 224, 224]),\n",
       " torch.Size([1173, 1, 224, 224]),\n",
       " torch.Size([1173, 1, 224, 224]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train,-1)\n",
    "Y_train = np.expand_dims(Y_train,-1)\n",
    "\n",
    "def elastic_transform(image, mask, alpha_affine):\n",
    "\n",
    "    random_state = np.random.RandomState(None)\n",
    "    shape = image.shape\n",
    "    shape_size = shape[:2]\n",
    "\n",
    "    # Random affine\n",
    "    center_square = np.float32(shape_size) // 2\n",
    "    square_size = 42#min(shape_size) // 3\n",
    "    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n",
    "    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n",
    "    M = cv2.getAffineTransform(pts1, pts2)\n",
    "    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "    mask = cv2.warpAffine(mask, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n",
    "    return image, mask\n",
    "\n",
    "def hflip_transform(image, mask):\n",
    "    image = cv2.flip(image, 1)\n",
    "    mask = cv2.flip(mask, 1)\n",
    "    return image, mask\n",
    "\n",
    "def vflip_transform(image, mask):\n",
    "    image = cv2.flip(image, 0)\n",
    "    mask = cv2.flip(mask, 0)\n",
    "    return image, mask\n",
    "\n",
    "def flip_transform(image, mask):\n",
    "    image = cv2.flip(image, -1)\n",
    "    mask = cv2.flip(mask, -1)\n",
    "    return image, mask\n",
    "\n",
    "el_train_images = [elastic_transform(img, mask, img.shape[1] * 0.04) for img,mask in zip(X_train,Y_train )]\n",
    "\n",
    "hflip_train_images = [hflip_transform(img, mask) for img,mask in zip(X_train,Y_train )]\n",
    "\n",
    "vflip_train_images = [vflip_transform(img, mask) for img,mask in zip(X_train,Y_train )]\n",
    "\n",
    "flip_train_images = [flip_transform(img, mask) for img,mask in zip(X_train,Y_train )]\n",
    "\n",
    "\n",
    "train_images_list = [el_train_images, hflip_train_images, vflip_train_images, flip_train_images]\n",
    "\n",
    "\n",
    "for each_el_tf in train_images_list:\n",
    "    el_tf_imgs,  el_tf_masks = zip(*each_el_tf)\n",
    "    el_tf_imgs = list(el_tf_imgs)\n",
    "    el_tf_masks = list(el_tf_masks)\n",
    "    X_train = np.concatenate((X_train, np.expand_dims(el_tf_imgs, -1)))\n",
    "    Y_train = np.concatenate((Y_train, np.expand_dims(el_tf_masks, -1)))\n",
    "\n",
    "X_train = torch.from_numpy(np.float32(np.squeeze(X_train,-1))).unsqueeze(1)\n",
    "Y_train = torch.from_numpy(np.float32(np.squeeze(Y_train,-1))).unsqueeze(1)\n",
    "X_valid = torch.from_numpy(np.float32(X_valid)).unsqueeze(1)\n",
    "Y_valid = torch.from_numpy(np.float32(Y_valid)).unsqueeze(1)\n",
    "X_test = torch.from_numpy(np.float32(X_test)).unsqueeze(1)\n",
    "Y_test = torch.from_numpy(np.float32(Y_test)).unsqueeze(1)\n",
    "X_train.shape, X_valid.shape, X_test.shape, Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e215bf86-ffe6-47f4-a4ab-939ccf484600",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(X_train,Y_train), batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(TensorDataset(X_valid,Y_valid), batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(TensorDataset(X_test,Y_test), batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a6beead-a7b3-45a3-bf9b-58c1719548fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    C = windows.shape[-1]\n",
    "    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n",
    "    return x\n",
    "\n",
    "def get_relative_position_index(win_h: int, win_w: int):\n",
    "    # get pair-wise relative position index for each token inside the window\n",
    "    coords = torch.stack(torch.meshgrid(torch.arange(win_h), torch.arange(win_w),indexing='ij'))  # 2, Wh, Ww\n",
    "    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "    relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n",
    "    relative_coords[:, :, 1] += win_w - 1\n",
    "    relative_coords[:, :, 0] *= 2 * win_w - 1\n",
    "    return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            window_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.window_area = self.window_size[0]*self.window_size[1]\n",
    "        self.num_heads = 4\n",
    "        head_dim =  dim // self.num_heads\n",
    "        # attn_dim = head_dim * self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size[0] - 1) **2, self.num_heads))\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        self.register_buffer(\"relative_position_index\", get_relative_position_index(self.window_size[0], self.window_size[1]), persistent=False)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        torch.nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def _get_rel_pos_bias(self):\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        return relative_position_bias.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn + self._get_rel_pos_bias()\n",
    "        if mask is not None:\n",
    "            num_win = mask.shape[0]\n",
    "            attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "        attn = self.softmax(attn)\n",
    "        x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B_, N, -1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51c8caa4-f837-41f8-a866-9c4cf451e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,  dim, input_resolution, window_size = 7, shift_size = 0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        window_size = (window_size, window_size)\n",
    "        shift_size = (shift_size, shift_size)\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.window_area = self.window_size[0] * self.window_size[1]\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(4 * dim),\n",
    "            nn.Linear( 4 * dim, dim)\n",
    "        )\n",
    "\n",
    "        if self.shift_size:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            H = math.ceil(H / self.window_size[0]) * self.window_size[0]\n",
    "            W = math.ceil(W / self.window_size[1]) * self.window_size[1]\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            cnt = 0\n",
    "            for h in (\n",
    "                    slice(0, -self.window_size[0]), \n",
    "                    slice(-self.window_size[0], -self.shift_size[0]),\n",
    "                    slice(-self.shift_size[0], None)):\n",
    "                for w in (\n",
    "                        slice(0, -self.window_size[1]),\n",
    "                        slice(-self.window_size[1], -self.shift_size[1]),\n",
    "                        slice(-self.shift_size[1], None)):\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_area)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask, persistent=False)\n",
    "\n",
    "    def _attn(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_area, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "        shifted_x = shifted_x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size:\n",
    "            x = torch.roll(shifted_x, shifts=self.shift_size, dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        B, H, W, C = x.shape\n",
    "        x = x + self._attn(self.norm1(x))\n",
    "        x = x.reshape(B, -1, C)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        x = x.reshape(B, H,W, C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffefa512-eb27-4b25-b4f6-18a468faf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_ch, num_feat, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch,num_feat, kernel_size=patch_size,\n",
    "                                  stride=patch_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Output shape: (batch size, no. of patches, no. of channels)\n",
    "        return self.conv(X).permute(0,2,3,1)\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(4 * dim)\n",
    "        self.reduction = nn.Linear(4*dim, 2*dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "class PatchExpansion(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim//2)\n",
    "        self.expand = nn.Linear(dim, 2*dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.expand(x)\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        x = x.view(B, H , W, 2, 2, C//4)\n",
    "        x = x.permute(0,1,3,2,4,5)\n",
    "\n",
    "        x = x.reshape(B,H*2, W*2 , C//4)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class FinalPatchExpansion(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.expand = nn.Linear(dim, 16*dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.expand(x)\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        x = x.view(B, H , W, 4, 4, C//16)\n",
    "        x = x.permute(0,1,3,2,4,5)\n",
    "\n",
    "        x = x.reshape(B,H*4, W*4 , C//16)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "933212db-98b4-48ab-ac1f-912f9b58fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dims, ip_res, ss_size = 3):\n",
    "        super().__init__()\n",
    "        self.swtb1 = SwinTransformerBlock(dim=dims, input_resolution=ip_res)\n",
    "        self.swtb2 = SwinTransformerBlock(dim=dims, input_resolution=ip_res, shift_size=ss_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.swtb2(self.swtb1(x))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, C, partioned_ip_res, num_blocks=3):\n",
    "        super().__init__()\n",
    "        H,W = partioned_ip_res[0], partioned_ip_res[1]\n",
    "        self.enc_swin_blocks = nn.ModuleList([\n",
    "            SwinBlock(C, (H, W)),\n",
    "            SwinBlock(2*C, (H//2, W//2)),\n",
    "            SwinBlock(4*C, (H//4, W//4))\n",
    "        ])\n",
    "        self.enc_patch_merge_blocks = nn.ModuleList([\n",
    "            PatchMerging(C),\n",
    "            PatchMerging(2*C),\n",
    "            PatchMerging(4*C)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_conn_ftrs = []\n",
    "        for swin_block,patch_merger in zip(self.enc_swin_blocks, self.enc_patch_merge_blocks):\n",
    "            x = swin_block(x)\n",
    "            skip_conn_ftrs.append(x)\n",
    "            x = patch_merger(x)\n",
    "        return x, skip_conn_ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, C, partioned_ip_res, num_blocks=3):\n",
    "        super().__init__()\n",
    "        H,W = partioned_ip_res[0], partioned_ip_res[1]\n",
    "        self.dec_swin_blocks = nn.ModuleList([\n",
    "            SwinBlock(4*C, (H//4, W//4)),\n",
    "            SwinBlock(2*C, (H//2, W//2)),\n",
    "            SwinBlock(C, (H, W))\n",
    "        ])\n",
    "        self.dec_patch_expand_blocks = nn.ModuleList([\n",
    "            PatchExpansion(8*C),\n",
    "            PatchExpansion(4*C),\n",
    "            PatchExpansion(2*C)\n",
    "        ])\n",
    "        self.skip_conn_concat = nn.ModuleList([\n",
    "            nn.Linear(8*C, 4*C),\n",
    "            nn.Linear(4*C, 2*C),\n",
    "            nn.Linear(2*C, 1*C)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for patch_expand,swin_block, enc_ftr, linear_concatter in zip(self.dec_patch_expand_blocks, self.dec_swin_blocks, encoder_features,self.skip_conn_concat):\n",
    "            x = patch_expand(x)\n",
    "            x = torch.cat([x, enc_ftr], dim=-1)\n",
    "            x = linear_concatter(x)\n",
    "            x = swin_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self, H, W, ch, C, num_class, num_blocks=3, patch_size = 4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(ch, C, patch_size)\n",
    "        self.encoder = Encoder(C, (H//patch_size, W//patch_size),num_blocks)\n",
    "        self.bottleneck = SwinBlock(C*(2**num_blocks), (H//(patch_size* (2**num_blocks)), W//(patch_size* (2**num_blocks))))\n",
    "        self.decoder = Decoder(C, (H//patch_size, W//patch_size),num_blocks)\n",
    "        self.final_expansion = FinalPatchExpansion(C)\n",
    "        self.head        = nn.Conv2d(C, num_class, 1,padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        x,skip_ftrs  = self.encoder(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoder(x, skip_ftrs[::-1])\n",
    "\n",
    "        x = self.final_expansion(x)\n",
    "\n",
    "        x = self.head(x.permute(0,3,1,2))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03fc5c1e-efcc-4605-bb54-ea1e39b4e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAVE_DIR = \"Your Directory\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x.to(device))\n",
    "        loss = loss_fn(out, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            out = model(x.to(device))\n",
    "            loss = loss_fn(out, y.to(device))\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Calculate Dice coefficient\n",
    "            pred = (out > 0.5).float()\n",
    "            intersection = (pred * y.to(device)).sum()\n",
    "            dice_score = (2. * intersection) / (pred.sum() + y.to(device).sum() + 1e-6)\n",
    "            dice_scores.append(dice_score.item())\n",
    "    return np.mean(losses), np.mean(dice_scores)\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, dice_scores, save_dir):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(dice_scores) + 1), dice_scores, label='Dice Coefficient', color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Dice Coefficient')\n",
    "    plt.title('Validation Dice Coefficient')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plots_save_path = \"your_path\"\n",
    "    plot_path = os.path.join(save_dir, plots_save_path)\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Metrics plot saved to: {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "def train(model, train_loader, valid_loader, optimizer, loss_fn, epochs, min_epochs, early_stop_count, device):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    dice_scores = []\n",
    "    best_valid_loss = float('inf')\n",
    "    EARLY_STOP = early_stop_count\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        valid_loss, dice_score = validate_epoch(model, valid_loader, loss_fn, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(valid_loss)\n",
    "        dice_scores.append(dice_score)\n",
    "\n",
    "        print(f\"Epoch: {ep}: train_loss={train_loss:.5f}, valid_loss={valid_loss:.5f}, dice_score={dice_score:.5f}\")\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            save_path = os.path.join(SAVE_DIR, f\"model_epoch_{ep}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': ep,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "            print(f\"Model checkpoint saved at: {save_path}\")\n",
    "\n",
    "        if ep > min_epochs:\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                best_model_path = os.path.join(SAVE_DIR, \"best_model.pth\")\n",
    "                torch.save({\n",
    "                    'epoch': ep,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, best_model_path)\n",
    "                print(f\"Best model saved with validation loss {valid_loss:.5f} at epoch {ep}\")\n",
    "                EARLY_STOP = early_stop_count\n",
    "            else:\n",
    "                EARLY_STOP -= 1\n",
    "                if EARLY_STOP <= 0:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    plot_metrics(train_losses, val_losses, dice_scores, SAVE_DIR)\n",
    "                    return train_losses, val_losses, dice_scores\n",
    "\n",
    "    # Save plots after the last epoch\n",
    "    plot_metrics(train_losses, val_losses, dice_scores, SAVE_DIR)\n",
    "    return train_losses, val_losses, dice_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ef739",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "model = SwinUNet(224,224,1,32,1,3,4).to(DEVICE)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "            nn.init.kaiming_uniform_(p)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=40,\n",
    "    min_epochs=5,\n",
    "    early_stop_count=10,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63863c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(3,4, figsize=(10,8))\n",
    "with torch.no_grad():\n",
    "    for i in range(3):\n",
    "        x_og,y_og = next(iter(train_loader))\n",
    "        x = x_og[0]\n",
    "        y = y_og[0]\n",
    "        \n",
    "        ax[i,0].imshow(x.squeeze(0).squeeze(0), cmap='gray')\n",
    "        ax[i,0].set_title('Image')\n",
    "        ax[i,1].imshow(y.squeeze(0).squeeze(0), cmap='gray')\n",
    "        ax[i,1].set_title('Mask')\n",
    "        x_og = x_og.to(DEVICE)\n",
    "        out = model(x_og[:1])\n",
    "        out = nn.Sigmoid()(out)\n",
    "        out = out.squeeze(0).squeeze(0).cpu()\n",
    "        ax[i,2].imshow(out, cmap='gray')\n",
    "        ax[i,2].set_title('Prediction')\n",
    "        ax[i,3].imshow((out>0.5).float(), cmap='gray')\n",
    "        ax[i,3].set_title('Threshold Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98effff-b0ee-42db-9d0a-981604d3a66d",
   "metadata": {},
   "source": [
    "## Dataset Testing for metrics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44319b62-8c7e-4214-93de-9eaff3e728d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94d50932-3377-4088-b733-7aacf1f710d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (size,size))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image = image/255.\n",
    "    return image\n",
    "\n",
    "def load_data(root_path, size):\n",
    "    images = []\n",
    "    masks = []\n",
    "    x = 0\n",
    "    for path in sorted(glob(root_path)):\n",
    "        img = load_image(path, size)\n",
    "        if 'mask' in path:\n",
    "            if x:\n",
    "                masks[-1] += img\n",
    "                masks[-1] = np.array(masks[-1]>0.5, dtype='float64')\n",
    "            else:\n",
    "                masks.append(img)\n",
    "                x = 1\n",
    "        else:\n",
    "            images.append(img)\n",
    "            x = 0\n",
    "    return np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f4358fd-5df9-4a37-a07c-6479d721cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def load_image(path, size):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.resize(image, (size, size))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "def load_data(image_folder, mask_folder, size):\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    image_paths = sorted(glob(os.path.join(image_folder, '*')))\n",
    "    mask_paths = sorted(glob(os.path.join(mask_folder, '*')))\n",
    "    \n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        img = load_image(img_path, size)\n",
    "        images.append(img)\n",
    "        \n",
    "        mask = load_image(mask_path, size)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return np.array(images), np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9ea3e198-8868-4d80-8fa1-ea2d318bf4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6732, 224, 224),\n",
       " (1443, 224, 224),\n",
       " (1443, 224, 224),\n",
       " (6732, 224, 224),\n",
       " (1443, 224, 224),\n",
       " (1443, 224, 224))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"Your designated mask path\"\n",
    "masks_path = \"Your designated mask path\"\n",
    "\n",
    "size = 224\n",
    "X, Y = load_data(image_path, masks_path, size)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, shuffle=True)\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size=0.5,shuffle=True)\n",
    "X_train.shape, X_valid.shape, X_test.shape, Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47b32649-fcc6-47b2-9e21-17e71e1b0207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6732, 1, 224, 224]),\n",
       " torch.Size([1443, 1, 224, 224]),\n",
       " torch.Size([1443, 1, 224, 224]),\n",
       " torch.Size([6732, 1, 224, 224]),\n",
       " torch.Size([1443, 1, 224, 224]),\n",
       " torch.Size([1443, 1, 224, 224]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train,-1)\n",
    "Y_train = np.expand_dims(Y_train,-1)\n",
    "\n",
    "X_train = torch.from_numpy(np.float32(np.squeeze(X_train,-1))).unsqueeze(1)\n",
    "Y_train = torch.from_numpy(np.float32(np.squeeze(Y_train,-1))).unsqueeze(1)\n",
    "\n",
    "X_valid = torch.from_numpy(np.float32(X_valid)).unsqueeze(1)\n",
    "Y_valid = torch.from_numpy(np.float32(Y_valid)).unsqueeze(1)\n",
    "X_test = torch.from_numpy(np.float32(X_test)).unsqueeze(1)\n",
    "Y_test = torch.from_numpy(np.float32(Y_test)).unsqueeze(1)\n",
    "\n",
    "X_train.shape, X_valid.shape, X_test.shape, Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ecfc4655-1d7d-4caf-a57b-6540f84ff138",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TensorDataset(X_train,Y_train), batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(TensorDataset(X_valid,Y_valid), batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(TensorDataset(X_test,Y_test), batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fac19b-fe35-4b33-9e88-d5ae15935744",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import jaccard_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "checkpoint = torch.load(\"Your Checkpoint.pth\", map_location=DEVICE)\n",
    "\n",
    "state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        new_state_dict[k[7:]] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "\n",
    "model = SwinUNet(224, 224, 1, 32, 1, 3, 4).to(DEVICE)\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()  \n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, mask_folder=None, size=224):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_folder, '*')))\n",
    "        self.mask_paths = sorted(glob(os.path.join(mask_folder, '*'))) if mask_folder else None\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (self.size, self.size))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = image / 255.0\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  \n",
    "        \n",
    "        if self.mask_paths:\n",
    "            mask_path = self.mask_paths[idx]\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, (self.size, self.size))\n",
    "            mask = mask / 255.0\n",
    "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "            return image, mask\n",
    "        \n",
    "        return image\n",
    "tput_folder =\"Output folder path\"  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "test_dataset = CustomDataset(image_folder, size=224)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, images in enumerate(test_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        outputs = nn.Sigmoid()(outputs)  \n",
    "        thresholded_outputs = (outputs > 0.5).float() \n",
    "        \n",
    "        for i in range(images.size(0)):\n",
    "            output_image = thresholded_outputs[i].cpu().squeeze(0)  \n",
    "            save_path = os.path.join(output_folder, f\"result_{batch_idx * test_loader.batch_size + i}.png\")\n",
    "            save_image(output_image, save_path)\n",
    "\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(\"Segmentation results saved successfully!\")\n",
    "image_folder = \"Image Folder path\"  \n",
    "ou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cb24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import jaccard_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "checkpoint = torch.load(\"Your model checkpoint.pth\", map_location=DEVICE)\n",
    "\n",
    "state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"module.\"):\n",
    "        new_state_dict[k[7:]] = v \n",
    "    else:\n",
    "        new_state_dict[k] = v\n",
    "\n",
    "model = SwinUNet(224, 224, 1, 32, 1, 3, 4).to(DEVICE)\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()  \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, size=224):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_folder, '*')))\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (self.size, self.size))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        image = image / 255.0\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0) \n",
    "        return image, os.path.basename(img_path)  \n",
    "\n",
    "image_folder = \"Image folder\"\n",
    "output_folder =\"Output folder\"  \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "test_dataset = CustomDataset(image_folder, size=224)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, filenames) in enumerate(test_loader):\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        outputs = nn.Sigmoid()(outputs)  \n",
    "        thresholded_outputs = (outputs > 0.5).float()  \n",
    "        \n",
    "        for i in range(images.size(0)):\n",
    "            output_image = thresholded_outputs[i].cpu().squeeze(0)  \n",
    "            save_path = os.path.join(output_folder, filenames[i])  \n",
    "            save_image(output_image, save_path)\n",
    "\n",
    "            print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(\"Segmentation results saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffa061-e0ea-41b8-b5fb-558d48a3021c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e819d7b-6af8-4328-9cfd-0f0bd960bec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e59813-2605-485b-ade2-4b810e50198f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
